---
title: "R Notebook"
output: html_notebook
---

```{r}
set.seed(42)
```


```{r}
# Function to initialize the parameters
initialize_parameters <- function(J) {
  list(
    # assume 50/50 probability of spam
    delta = 0.3,
    
    # assume spam words have high probability and ham words have low probability given message is spam
    p1 = runif(J, 0.4, 0.6),
    
    # assume all words have low probability given message is not spam
    p0 = runif(J, 0.4, 0.6)
  )
}
```


```{r}
# calculate gamma
calc_gamma <- function(Y, params) {
  N <- nrow(Y)
  J <- ncol(Y)
  delta <- params$delta
  p1 <- params$p1
  p0 <- params$p0
  
  gamma <- numeric(N)  # responsibilities (spam probabilities for each message)
  
  for (i in 1:N) {
    # Likelihood of message i given it's spam (X_i = 1)
    prob_spam <- delta * prod(p1^Y[i, ] * (1 - p1)^(1 - Y[i, ]))
    
    # Likelihood of message i given it's non-spam (X_i = 0)
    prob_non_spam <- (1 - delta) * prod(p0^Y[i, ] * (1 - p0)^(1 - Y[i, ]))
    
    # Posterior probability of spam (gamma_i)
    gamma[i] <- prob_spam / (prob_spam + prob_non_spam)
  }
  
  return(gamma)
}
```


```{r}
# E-step: Calculate responsibilities (posterior probabilities that each message is spam)
e_step <- function(Y, params) {
  gamma <- calc_gamma(Y, params)
  return(gamma)
}
```


```{r}
# M-step: Update parameters based on the current responsibilities
m_step <- function(Y, gamma) {
  N <- nrow(Y)
  J <- ncol(Y)
  
  # Update delta (prior probability of spam)
  delta <- mean(gamma)
  
  # Update p1 (word probabilities for spam messages)
  p1 <- colSums(gamma * Y) / sum(gamma)
  
  # Update p0 (word probabilities for non-spam messages)
  p0 <- colSums((1 - gamma) * Y) / sum(1 - gamma)
  
  return(list(delta = delta, p1 = p1, p0 = p0))
}
```


```{r}
# Function to calculate the log-likelihood (for monitoring convergence)
log_likelihood <- function(Y, params) {
  N <- nrow(Y)
  J <- ncol(Y)
  delta <- params$delta
  p1 <- params$p1
  p0 <- params$p0
  
  logL <- 0
  for (i in 1:N) {
    prob_spam <- delta * prod(p1^Y[i, ] * (1 - p1)^(1 - Y[i, ]))
    prob_non_spam <- (1 - delta) * prod(p0^Y[i, ] * (1 - p0)^(1 - Y[i, ]))
    
    logL <- logL + log(prob_spam + prob_non_spam)
  }
  
  return(logL)
}
```


```{r}
# Main function to run the EM algorithm
em_algorithm <- function(Y) {
  N <- nrow(Y)
  J <- ncol(Y)
  
  # Initialize the parameters
  params <- initialize_parameters(J)
  logLikelihood_old <- Inf
  logLikelihood_new <- -Inf
  
  iter <- 0
  tol = 1e-8
  
  while(abs(logLikelihood_old - logLikelihood_new) > tol) {
    iter <- iter + 1
    
    # E-step: Compute responsibilities (posterior probabilities)
    gamma <- e_step(Y, params)
    
    # M-step: Update parameters
    params <- m_step(Y, gamma)
    
    # Compute log-likelihood
    logLikelihood_old <- logLikelihood_new
    logLikelihood_new <- log_likelihood(Y, params)
    
    if(iter == 1 | iter %% 20 == 0) {
      cat('Iteration:', iter, 'logL:', logLikelihood_new, 'delta:', params$delta, '\n',
          'p1,j:', params$p1, '\n',
          'p0,j:', params$p0, '\n')
      cat('\n')
    }
  }
  
  return(list(params = params, log_likelihood = logLikelihood_new, gamma = gamma))
}
```


Read data
```{r}
data <- read.csv("Spam_BagOfWord.csv")
data
```


```{r}
train <- data[1:length(data$message), ]

train_label <- train$label

train_sub <- subset(train, select=-c(message, label))

words <- colnames(train_sub)
Y_train <- as.matrix(train_sub)
```


Calculate
```{r}
result <- em_algorithm(Y_train)

cat("Learned Parameters:\n")
print(lapply(result$params, round, 3))

cat("\nLog-Likelihood:\n")
print(result$log_likelihood)
```


```{r}
cat('% of spam', mean(train$label), '\n')


# benchmark
predicted_labels <- ifelse(runif(5572, 0, 1) < initialize_parameters(length(words))$delta, 1, 0)

# Calculate precision
true_positives <- sum(predicted_labels == 1 & train_label == 1)
false_positives <- sum(predicted_labels == 1 & train_label == 0)
false_negative <- sum(predicted_labels == 0 & train_label == 1)
true_negative <- sum(predicted_labels == 0 & train_label == 0)

accuracy <- (true_positives + true_negative)/(true_positives + false_positives + false_negative + true_negative)
precision <- true_positives/(true_positives + false_positives)
recall <- true_positives/(true_positives + false_negative)
F1 <- 2*precision*recall/(precision+recall)

cat("\n")
cat("Benchmark", "\n")
cat('Predicted % of spam', mean(predicted_labels), '\n')
cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1:", F1, "\n")


# Assign labels based on the posterior probability
train_gamma <- calc_gamma(Y_train, result$params)
predicted_labels <- ifelse(train_gamma > 0.5, 1, 0)

# Calculate precision
true_positives <- sum(predicted_labels == 1 & train_label == 1)
false_positives <- sum(predicted_labels == 1 & train_label == 0)
false_negative <- sum(predicted_labels == 0 & train_label == 1)
true_negative <- sum(predicted_labels == 0 & train_label == 0)

accuracy <- (true_positives + true_negative)/(true_positives + false_positives + false_negative + true_negative)
precision <- true_positives/(true_positives + false_positives)
recall <- true_positives/(true_positives + false_negative)
F1 <- 2*precision*recall/(precision+recall)

cat("\n")
cat('Predicted % of spam', mean(predicted_labels), '\n')
cat("Naive Bayes", "\n")
cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1:", F1, "\n")
```
True positive
```{r}
train[(predicted_labels == 1) & (train_label == 1), ]
```

False positive
```{r}
train[(predicted_labels == 1) & (train_label == 0), ]
```


```{r}
train[(predicted_labels == 0) & (train_label == 1), ]
```




